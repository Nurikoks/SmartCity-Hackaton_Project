import os
import re
import math
import json
import pandas as pd
from typing import List, Tuple

#–ò–ú–ü–û–†–¢–´ –î–õ–Ø RAG (Chroma + SentenceTransformer) 
try:
    from langchain_community.vectorstores import Chroma
    from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
    from langchain_core.documents import Document
    HAS_RAG_LIBS = True
except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ RAG –±–∏–±–ª–∏–æ—Ç–µ–∫: {e}")
    HAS_RAG_LIBS = False

#–ò–ú–ü–û–†–¢ –î–õ–Ø GEMINI (LLM)
try:
    import google.generativeai as genai
    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
    if GEMINI_API_KEY:
        genai.configure(api_key=GEMINI_API_KEY)
        HAS_GEMINI = True
    else:
        HAS_GEMINI = False
except ImportError:
    genai = None
    HAS_GEMINI = False

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ RAG
VECTOR_DB_PATH = "./chroma_db"
CLEAN_DATA_PATH = "data/gis_clean.csv"
EMBEDDING_MODEL = "all-MiniLM-L6-v2" 

#SYSTEM_PROMPT 
SYSTEM_PROMPT = """
–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç-–≥–∏–¥ –ø–æ –ø—Ä–æ–≥—É–ª–∫–∞–º –≤ –≥–æ—Ä–æ–¥–µ –ê—Å—Ç–∞–Ω–∞. 
–¢–≤–æ—è —Ü–µ–ª—å ‚Äî –¥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é 1-3 —Å–∞–º—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏ –≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.

---
[–ö–û–ù–¢–ï–ö–°–¢ POI]
{context}

---
–ò–ù–°–¢–†–£–ö–¶–ò–ò (–°–õ–ï–î–û–í–ê–¢–¨ –°–¢–†–û–ì–û):

1. –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨: –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–ª–æ–∫–∞ [–ö–û–ù–¢–ï–ö–°–¢ POI].
2. –§–û–†–ú–ê–¢: –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º –∏ –≤ —Ñ–æ—Ä–º–µ —Å–ø–∏—Å–∫–∞ (1-3 –ø—É–Ω–∫—Ç–∞). –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—É–Ω–∫—Ç–∞:
   - –ù–∞–∑–≤–∞–Ω–∏–µ, –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–≤ –º–µ—Ç—Ä–∞—Ö) –∏ –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –Ω–∞ –ø–æ—Å–µ—â–µ–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å).
   - –ü–û–ß–ï–ú–£: –ö—Ä–∞—Ç–∫–æ–µ —É–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ (–º–æ—Ç–∏–≤–∞—Ü–∏—è).
   - –ü–õ–ê–ù –î–ï–ô–°–¢–í–ò–ô: –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –∏–¥–µ—è "—á—Ç–æ —Ç–∞–º —Å–¥–µ–ª–∞—Ç—å" (–º–∏–∫—Ä–æ-–º–∞—Ä—à—Ä—É—Ç/—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è).
3. –ü–†–ê–í–ò–õ–ê: 
   - –Ø—Å–Ω–æ —É–∫–∞–∂–∏, –∫–∞–∫–∏–µ –§–ò–õ–¨–¢–†–´ –±—ã–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã (–ª–æ–∫–∞—Ü–∏—è, –∫–∞—Ç–µ–≥–æ—Ä–∏—è).
   - –ò–∑–±–µ–≥–∞–π —É–≤–µ—Ä–µ–Ω–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ ('–æ—Ç–∫—Ä—ã—Ç–æ —Å–µ–π—á–∞—Å'), –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç.
"""

def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """–†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–æ—á–∫–∞–º–∏ –Ω–∞ –ó–µ–º–ª–µ (–≤ –∫–∏–ª–æ–º–µ—Ç—Ä–∞—Ö)."""
    R = 6371  # —Ä–∞–¥–∏—É—Å –ó–µ–º–ª–∏ –≤ –∫–º
    d_lat = math.radians(lat2 - lat1)
    d_lon = math.radians(lon2 - lon1)
    a = (
        math.sin(d_lat / 2) ** 2
        + math.cos(math.radians(lat1))
        * math.cos(math.radians(lat2))
        * math.sin(d_lon / 2) ** 2
    )
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    return R * c


def index_data_to_chroma(data_source_path: str = CLEAN_DATA_PATH):
    """
    –§–£–ù–ö–¶–ò–Ø –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–ò: –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ, —Å–æ–∑–¥–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –≤ ChromaDB.
    –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è/–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î.
    """
    if not HAS_RAG_LIBS:
        print("‚ö†Ô∏è –ú–æ–¥—É–ª–∏ LangChain/ChromaDB –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –ü—Ä–æ–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
        return

    if not os.path.exists(data_source_path):
        print(f"‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {data_source_path}")
        return

    df = pd.read_csv(data_source_path, sep=";", dtype=str)
    documents = []

    for _, row in df.iterrows():
        # –ö–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        content = (
            f"–ù–∞–∑–≤–∞–Ω–∏–µ: {row.get('–ù–∞–∑–≤–∞–Ω–∏–µ', '')}. "
            f"–†—É–±—Ä–∏–∫–∞: {row.get('–†—É–±—Ä–∏–∫–∞', '')} ({row.get('–ü–æ–¥—Ä—É–±—Ä–∏–∫–∞', '')}). "
            f"–ê–¥—Ä–µ—Å: {row.get('–ê–¥—Ä–µ—Å', '')}. "
            f"–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {row.get('–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', '')}."
        )

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é
        try:
            lat = float(row.get("–®–∏—Ä–æ—Ç–∞", "0") or 0)
            lon = float(row.get("–î–æ–ª–≥–æ—Ç–∞", "0") or 0)
        except ValueError:
            lat, lon = 0.0, 0.0

        metadata = {
            "–ù–∞–∑–≤–∞–Ω–∏–µ": row.get("–ù–∞–∑–≤–∞–Ω–∏–µ"),
            "–®–∏—Ä–æ—Ç–∞": lat,
            "–î–æ–ª–≥–æ—Ç–∞": lon,
            "–ê–¥—Ä–µ—Å": row.get("–ê–¥—Ä–µ—Å"),
            "–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã": row.get("–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã"),
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": row.get("–ü–æ–¥—Ä—É–±—Ä–∏–∫–∞"),
        }

        documents.append(Document(page_content=content, metadata=metadata))

    embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)

    db = Chroma.from_documents(
        documents,
        embeddings,
        persist_directory=VECTOR_DB_PATH,
    )

    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {len(df)} –∑–∞–ø–∏—Å–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ë–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {VECTOR_DB_PATH}.")

#–ú–ê–ü–ü–ò–ù–ì –ü–û–î–†–£–ë–†–ò–ö –ù–ê –ö–ê–¢–ï–ì–û–†–ò–ò (–ø–æ —Ä–µ–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º)
SUBRUBRIKA_TO_INTENT = {
    # coffee
    "–ö–æ—Ñ–µ–π–Ω–∏": "coffee",
    "–ö–∞—Ñ–µ": "coffee",
    "–ß–∞–π–Ω—ã–µ": "coffee",
    "–ö–æ–Ω–¥–∏—Ç–µ—Ä—Å–∫–∏–µ": "coffee",

    # restaurant
    "–†–µ—Å—Ç–æ—Ä–∞–Ω—ã": "restaurant",
    "–§–∞—Å—Ç—Ñ—É–¥": "restaurant",
    "–ü–∏—Ü—Ü–µ—Ä–∏–∏": "restaurant",
    "–°—É—à–∏-–±–∞—Ä—ã": "restaurant",
    "–ë–∞—Ä—ã": "restaurant",
    "–ü–∞–±—ã": "restaurant",
    "–°—Ç–æ–ª–æ–≤—ã–µ": "restaurant",

    # clinic
    "–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ü–µ–Ω—Ç—Ä—ã": "clinic",
    "–ü–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∏": "clinic",
    "–ë–æ–ª—å–Ω–∏—Ü—ã": "clinic",
    "–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—Ç—Ä—ã": "clinic",
    "–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–∏–∏": "clinic",

    # dentist
    "–°—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏": "dentist",
    "–°—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–ª–∏–Ω–∏–∫–∏": "dentist",
    "–û—Ä—Ç–æ–¥–æ–Ω—Ç—ã": "dentist",

    # pharmacy
    "–ê–ø—Ç–µ–∫–∏": "pharmacy",
    "–õ–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞": "pharmacy",
    "–í–∏—Ç–∞–º–∏–Ω—ã": "pharmacy",

    # bank
    "–ë–∞–Ω–∫–∏": "bank",
    "–ë–∞–Ω–∫–æ–º–∞—Ç—ã": "bank",
    "–õ–æ–º–±–∞—Ä–¥—ã": "bank",
    "–û–±–º–µ–Ω –≤–∞–ª—é—Ç": "bank",
    "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —É—Å–ª—É–≥–∏": "bank",

    # fitness
    "–§–∏—Ç–Ω–µ—Å-—Ü–µ–Ω—Ç—Ä—ã": "fitness",
    "–¢—Ä–µ–Ω–∞–∂—ë—Ä–Ω—ã–µ –∑–∞–ª—ã": "fitness",
    "–ô–æ–≥–∞-—Å—Ç—É–¥–∏–∏": "fitness",
    "–°–ø–æ—Ä—Ç–∏–≤–Ω—ã–µ –∫–ª—É–±—ã": "fitness",
    "SPA": "fitness",

    # beauty
    "–°–∞–ª–æ–Ω—ã –∫—Ä–∞—Å–æ—Ç—ã": "beauty",
    "–ü–∞—Ä–∏–∫–º–∞—Ö–µ—Ä—Å–∫–∏–µ": "beauty",
    "–ú–∞–Ω–∏–∫—é—Ä": "beauty",
    "–ü–µ–¥–∏–∫—é—Ä": "beauty",
    "–ö–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—è": "beauty",
    "–ë–∞—Ä–±–µ—Ä—à–æ–ø—ã": "beauty",

    # shopping
    "–¢–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–Ω—Ç—Ä—ã": "shopping",
    "–ú–∞–≥–∞–∑–∏–Ω—ã": "shopping",
    "–ë—É—Ç–∏–∫–∏": "shopping",
    "–°—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç—ã": "shopping",
    "–ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ": "shopping",

    # cinema
    "–ö–∏–Ω–æ—Ç–µ–∞—Ç—Ä—ã": "cinema",
    "–†–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–Ω—Ç—Ä—ã": "cinema",
    "–¢–µ–∞—Ç—Ä—ã": "cinema",
}



def search_poi_with_filter(
    query: str,
    user_lat: float,
    user_lon: float,
    max_distance_km: float = 5.0,
    top_k: int = 500,
) -> str:
    """
    –ü–æ–∏—Å–∫ POI:
    1) –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞ (coffee, cinema, restaurant –∏ —Ç.–¥.).
    2) –ë–µ—Ä—ë–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Chroma.
    3) –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (SUBRUBRIKA_TO_INTENT).
    4) –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—Ä–∞—Å–∏–≤–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è Telegram.
    """

    if not HAS_RAG_LIBS or not os.path.exists(VECTOR_DB_PATH):
        return (
            f"[–ó–ê–ì–õ–£–®–ö–ê] –ë–∞–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ Chroma –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.\n"
            f"–ó–∞–ø—Ä–æ—Å: {query}\n"
            f"–õ–æ–∫–∞—Ü–∏—è: ({user_lat}, {user_lon})"
        )

    embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)
    db = Chroma(
        persist_directory=VECTOR_DB_PATH,
        embedding_function=embeddings,
    )
    relevant_docs = db.similarity_search_with_score(query, k=top_k)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç –ø–æ –∑–∞–ø—Ä–æ—Å—É
    q_lower = (query or "").lower()
    # –¢–æ–∫–µ–Ω—ã –∑–∞–ø—Ä–æ—Å–∞: ["–º–Ω–µ", "–Ω—É–∂–Ω–µ–Ω", "–∫–∏–Ω–æ", "–Ω–µ–¥–∞–ª–µ–∫–æ"]
    query_tokens = re.findall(r"\w+", q_lower)

    intent_keywords = {
        "coffee":     ["–∫–æ—Ñ–µ", "–∫–∞—Ñ–µ", "coffee", "–∫–∞–ø—É—á–∏–Ω–æ", "–ª–∞—Ç—Ç–µ"],
        "restaurant": ["—Ä–µ—Å—Ç–æ—Ä–∞–Ω", "—Ä–µ—Å—Ç–æ—Ä–∞–Ω—á–∏–∫", "—Ñ–∞—Å—Ç—Ñ—É–¥", "–±—É—Ä–≥–µ—Ä", "–ø–∏—Ü—Ü–∞", "—Å—É—à–∏", "–¥–æ–Ω–µ—Ä"],
        "dentist":    ["—Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥", "—Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—è", "–∑—É–±", "–∑—É–±—ã", "–æ—Ä—Ç–æ–¥–æ–Ω—Ç", "–ø–ª–æ–º–±–∞"],
        "pharmacy":   ["–∞–ø—Ç–µ–∫–∞", "–ª–µ–∫–∞—Ä—Å—Ç–≤–æ", "–ª–µ–∫–∞—Ä—Å—Ç–≤–∞", "–≤–∏—Ç–∞–º–∏–Ω", "–≤–∏—Ç–∞–º–∏–Ω—ã"],
        "clinic":     ["–∫–ª–∏–Ω–∏–∫–∞", "–±–æ–ª—å–Ω–∏—Ü–∞", "–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞", "–º–µ–¥—Ü–µ–Ω—Ç—Ä", "–≤—Ä–∞—á"],
        "bank":       ["–±–∞–Ω–∫", "–±–∞–Ω–∫–æ–º–∞—Ç", "–ª–æ–º–±–∞—Ä–¥", "–∫—Ä–µ–¥–∏—Ç"],
        "beauty":     ["—Å–∞–ª–æ–Ω", "—Å–∞–ª–æ–Ω—ã", "–º–∞–Ω–∏–∫—é—Ä", "–ø–µ–¥–∏–∫—é—Ä", "–ø–∞—Ä–∏–∫–º–∞—Ö–µ—Ä—Å–∫–∞—è", "–±–∞—Ä–±–µ—Ä—à–æ–ø"],
        "fitness":    ["—Ñ–∏—Ç–Ω–µ—Å", "—Å–ø–æ—Ä—Ç–∑–∞–ª", "—Å–ø–æ—Ä—Ç", "–π–æ–≥–∞", "gym", "—Ç—Ä–µ–Ω–∞–∂–µ—Ä–Ω—ã–π"],
        "shopping":   ["–º–∞–≥–∞–∑–∏–Ω", "–º–∞—Ä–∫–µ—Ç", "—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä", "–±—É—Ç–∏–∫", "—Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç"],
        "cinema":     ["–∫–∏–Ω–æ", "–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä", "cinema", "—Ñ–∏–ª—å–º"],
    }

    def matches_intent_in_query(intent_words, tokens, full_text_lower) -> bool:
        """
        –î–ª—è –æ–¥–Ω–æ—Å–ª–æ–≤–Ω—ã—Ö –∫–ª—é—á–µ–π ‚Äî –∏—â–µ–º —Å—Ä–µ–¥–∏ —Ç–æ–∫–µ–Ω–æ–≤.
        –î–ª—è –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã—Ö ("—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä") ‚Äî –∏—â–µ–º –ø–æ–¥—Å—Ç—Ä–æ–∫—É –≤ –ø–æ–ª–Ω–æ–º —Ç–µ–∫—Å—Ç–µ.
        """
        for kw in intent_words:
            kw = kw.lower()
            if " " in kw:
                # —Ñ—Ä–∞–∑–∞
                if kw in full_text_lower:
                    return True
            else:
                # –æ–¥–∏–Ω–æ—á–Ω–æ–µ —Å–ª–æ–≤–æ
                if kw in tokens:
                    return True
        return False

    matched_intent = None
    for intent, words in intent_keywords.items():
        if matches_intent_in_query(words, query_tokens, q_lower):
            matched_intent = intent
            break

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é POI –ø–æ –ü–æ–¥—Ä—É–±—Ä–∏–∫–µ
    def detect_poi_intent(doc):
        """
        –ë–µ—Ä—ë–º doc.metadata["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"], –≤ –∫–æ—Ç–æ—Ä–æ–π –ª–µ–∂–∏—Ç –ü–æ–¥—Ä—É–±—Ä–∏–∫–∞.
        –ü—Ä–∏–º–µ—Ä: '–ë–∞–Ω–∫–µ—Ç–Ω—ã–µ –∑–∞–ª—ã, –ë–∞—Ä—ã, –†–µ—Å—Ç–æ—Ä–∞–Ω—ã'
        –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º –∏ –ø—ã—Ç–∞–µ–º—Å—è —Å–º–∞—Ç—á–∏—Ç—å –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å —Å SUBRUBRIKA_TO_INTENT.
        """
        cat_raw = (doc.metadata.get("–ö–∞—Ç–µ–≥–æ—Ä–∏—è") or "").strip().lower()
        if not cat_raw:
            return None

        # '–±–∞–Ω–∫–µ—Ç–Ω—ã–µ –∑–∞–ª—ã, –±–∞—Ä—ã, —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã' -> ['–±–∞–Ω–∫–µ—Ç–Ω—ã–µ –∑–∞–ª—ã', '–±–∞—Ä—ã', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã']
        parts = [c.strip() for c in cat_raw.split(",") if c.strip()]

        for part in parts:
            for subr, intent in SUBRUBRIKA_TO_INTENT.items():
                s = subr.lower()
                # –Ω–µ–º–Ω–æ–≥–æ "–º—è–≥–∫–æ–µ" —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
                if s in part or part in s:
                    return intent

        return None

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    # –±—É–¥–µ–º —Ö—Ä–∞–Ω–∏—Ç—å: (distance_km, name, category, address, time)
    pois: list[tuple[float, str, str, str, str]] = []

    for doc, score in relevant_docs:
        try:
            poi_lat = float(doc.metadata.get("–®–∏—Ä–æ—Ç–∞", 0) or 0)
            poi_lon = float(doc.metadata.get("–î–æ–ª–≥–æ—Ç–∞", 0) or 0)
        except ValueError:
            continue

        distance_km = haversine_distance(user_lat, user_lon, poi_lat, poi_lon)
        if distance_km > max_distance_km:
            continue

        poi_intent = detect_poi_intent(doc)

        # –µ—Å–ª–∏ –∏–Ω—Ç–µ–Ω—Ç –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω ‚Äî —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ –Ω–µ–º—É
        if matched_intent is not None and poi_intent is not None:
            if poi_intent != matched_intent:
                continue
        elif matched_intent is not None and poi_intent is None:
            # —Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º: –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —è–≤–Ω–æ –ø—Ä–æ—Å–∏–ª –∫–∏–Ω–æ, –∞ —É POI –Ω–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            continue

        name = doc.metadata.get("–ù–∞–∑–≤–∞–Ω–∏–µ") or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
        cat = doc.metadata.get("–ö–∞—Ç–µ–≥–æ—Ä–∏—è") or ""
        addr = doc.metadata.get("–ê–¥—Ä–µ—Å") or "–ê–¥—Ä–µ—Å –Ω–µ —É–∫–∞–∑–∞–Ω"
        time = doc.metadata.get("–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã") or "–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –Ω–µ —É–∫–∞–∑–∞–Ω–æ"

        pois.append((distance_km, name, cat, addr, time))

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ 
    if not pois:
        if matched_intent:
            return (
                f"–Ø –∏—Å–∫–∞–ª –º–µ—Å—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{matched_intent}' "
                f"–≤ —Ä–∞–¥–∏—É—Å–µ {max_distance_km:.1f} –∫–º, –Ω–æ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à—ë–ª.\n"
                "–ü–æ–ø—Ä–æ–±—É–π –∏–∑–º–µ–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–¥–∏—É—Å üôÇ"
            )
        else:
            return (
                f"–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–æ—Å—å –≤ —Ä–∞–¥–∏—É—Å–µ {max_distance_km:.1f} –∫–º.\n"
                "–ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å üôÇ"
            )

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –∫—Ä–∞—Å–∏–≤–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è —Ç–µ–ª–µ–≥–∏
    pois.sort(key=lambda x: x[0])
    top_pois = pois[:5]

    total_found = len(pois)
    header_lines = []

    header_lines.append(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {total_found} –º–µ—Å—Ç –≤ —Ä–∞–¥–∏—É—Å–µ {max_distance_km:.1f} –∫–º.")
    if matched_intent:
        header_lines.append(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {matched_intent}")
    header = "\n".join(header_lines)

    lines = [header, ""]  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞

    for idx, (dist, name, cat, addr, time) in enumerate(top_pois, start=1):
        block = [
            f"{idx}) {name}",
            f"   üìç {addr}",
            f"   üìè {dist:.2f} –∫–º",
            f"   üè∑ {cat}",
            f"   üïí {time}",
        ]
        lines.append("\n".join(block))
        lines.append("")  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É –∫–∞—Ä—Ç–æ—á–∫–∞–º–∏

    pretty_text = "\n".join(lines).strip()
    return pretty_text

def get_final_recommendation(user_query: str, user_lat: float, user_lon: float) -> str:
    """
    –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è Telegram-–±–æ—Ç–∞:
    - –¥–µ–ª–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ POI (Chroma + Haversine),
    - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç –Ω–∞–ø—Ä—è–º—É—é, –±–µ–∑ –≤—ã–∑–æ–≤–∞ LLM.
    """
    context = search_poi_with_filter(user_query, user_lat, user_lon)
    return context


# –¢–æ–ª—å–∫–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
# if __name__ == "__main__":
    test_query = "–º–Ω–µ –Ω—É–∂–Ω–∞ –∞–ø—Ç–µ–∫–∞ –Ω–µ–¥–∞–ª–µ–∫–æ"
    user_lat = 51.1095   
    user_lon = 71.5260

    print("--- –¢–ï–°–¢: –ü–û–ò–°–ö POI ---")
    print(search_poi_with_filter(test_query, user_lat, user_lon))

    # –†–ê–°–°–ö–û–ú–ï–ù–¢–ò–†–û–í–ê–¢–¨ –ü–û–°–õ–ï –°–û–ó–î–ê–ù–ò–Ø –ë–î–®–ö–ò!!!
    # print("\n--- –¢–ï–°–¢: –ò–ù–î–ï–ö–°–ê–¶–ò–Ø ---")
    # index_data_to_chroma(CLEAN_DATA_PATH)

